syntax = "proto3";
package speech_to_text;

import "google/api/annotations.proto";
import "google/api/client.proto";
import "google/api/field_behavior.proto";
import "google/rpc/status.proto";

option java_multiple_files = true;
option java_outer_classname = "SpeechToTextProto";
option java_package = "ai.vernacular.speech";

service SpeechToText {
  // Performs synchronous non-streaming speech recognition
  rpc Recognize(RecognizeRequest) returns (RecognizeResponse) {
    option (google.api.http) = {
      post: "/v1/speech:recognize"
      body: "*"
    };
    option (google.api.method_signature) = "config,audio";
  }

  // Performs bidirectional streaming speech recognition: receive results while
  // sending audio. This method is only available via the gRPC API (not REST).
  rpc StreamingRecognize(stream StreamingRecognizeRequest) returns (stream StreamingRecognizeResponse) {}

  // Performs asynchronous non-streaming speech recognition
  rpc LongRunningRecognize(LongRunningRecognizeRequest) returns (SpeechOperation) {}
  // Returns SpeechOperation for LongRunningRecognize. Used for polling the result
  rpc GetSpeechOperation(SpeechOperationRequest) returns (SpeechOperation) {
    option (google.api.http) = {
      get: "/v1/speech_operations/{name}"
    };
  }
}

//--------------------------------------------
// requests
//--------------------------------------------
message RecognizeRequest {
  // Required. Provides information to the recognizer that specifies how to
  // process the request.
  RecognitionConfig config = 1 [(google.api.field_behavior) = REQUIRED];

  // Required. The audio data to be recognized.
  RecognitionAudio audio = 2 [(google.api.field_behavior) = REQUIRED];

  string segment = 16;
}

message LongRunningRecognizeRequest {
  // Required. Provides information to the recognizer that specifies how to
  // process the request.
  RecognitionConfig config = 1 [(google.api.field_behavior) = REQUIRED];

  // Required. The audio data to be recognized.
  RecognitionAudio audio = 2 [(google.api.field_behavior) = REQUIRED];

  // Optional. When operation completes, result is posted to this url if provided.
  string result_url = 11;

  string segment = 16;
}

message SpeechOperationRequest {
  // name of the speech operation
  string name = 1 [(google.api.field_behavior) = REQUIRED];
}

message StreamingRecognizeRequest {
  // The streaming request, which is either a streaming config or audio content.
  oneof streaming_request {
    // Provides information to the recognizer that specifies how to process the
    // request. The first `StreamingRecognizeRequest` message must contain a
    // `streaming_config`  message.
    StreamingRecognitionConfig streaming_config = 1;

    // The audio data to be recognized.
    bytes audio_content = 2;
  }
}

message StreamingRecognitionConfig {
  // Required. Provides information to the recognizer that specifies how to
  // process the request.
  RecognitionConfig config = 1 [(google.api.field_behavior) = REQUIRED];

  // If `true`, interim results (tentative hypotheses) may be
  // returned as they become available (these interim results are indicated with
  // the `is_final=false` flag).
  // If `false` or omitted, only `is_final=true` result(s) are returned.
  bool interim_results = 2;
}

// Provides information to the recognizer that specifies how to process the request
message RecognitionConfig {
  enum AudioEncoding {
    ENCODING_UNSPECIFIED = 0;
    LINEAR16 = 1;
    FLAC = 2;
    MP3 = 3;
  }

  AudioEncoding encoding = 1;
  int32 sample_rate_hertz = 2; // Valid values are: 8000-48000.
  string language_code = 3 [(google.api.field_behavior) = REQUIRED];
  int32 max_alternatives = 4;
  repeated SpeechContext speech_contexts = 5;
  int32 audio_channel_count = 6;
  bool enable_separate_recognition_per_channel = 7;
  bool enable_word_time_offsets = 8;
  bool enable_automatic_punctuation = 11;
  SpeakerDiarizationConfig diarization_config = 16;
}

message SpeechContext {
  repeated string phrases = 1;
}

// Config to enable speaker diarization.
message SpeakerDiarizationConfig {
  // If 'true', enables speaker detection for each recognized word in
  // the top alternative of the recognition result using a speaker_tag provided
  // in the WordInfo.
  bool enable_speaker_diarization = 1;

  // Minimum number of speakers in the conversation. This range gives you more
  // flexibility by allowing the system to automatically determine the correct
  // number of speakers. If not set, the default value is 2.
  int32 min_speaker_count = 2;

  // Maximum number of speakers in the conversation. This range gives you more
  // flexibility by allowing the system to automatically determine the correct
  // number of speakers. If not set, the default value is 6.
  int32 max_speaker_count = 3;
}

// Either `content` or `uri` must be supplied.
message RecognitionAudio {
  oneof audio_source {
    bytes content = 1;
    string uri = 2;
  }
}

//--------------------------------------------
// responses
//--------------------------------------------
message RecognizeResponse {
  repeated SpeechRecognitionResult results = 1;
}

message LongRunningRecognizeResponse {
  repeated SpeechRecognitionResult results = 1;
}

message StreamingRecognizeResponse {
  // If set, returns a [google.rpc.Status][google.rpc.Status] message that
  // specifies the error for the operation.
  google.rpc.Status error = 1;

  // This repeated list contains zero or more results that
  // correspond to consecutive portions of the audio currently being processed.
  // It contains zero or one `is_final=true` result (the newly settled portion),
  // followed by zero or more `is_final=false` results (the interim results).
  repeated StreamingRecognitionResult results = 2;
}

message SpeechRecognitionResult {
  repeated SpeechRecognitionAlternative alternatives = 1;
  int32 channel_tag = 2;
}

message StreamingRecognitionResult {
  // May contain one or more recognition hypotheses (up to the
  // maximum specified in `max_alternatives`).
  // These alternatives are ordered in terms of accuracy, with the top (first)
  // alternative being the most probable, as ranked by the recognizer.
  repeated SpeechRecognitionAlternative alternatives = 1;

  // If `false`, this `StreamingRecognitionResult` represents an
  // interim result that may change. If `true`, this is the final time the
  // speech service will return this particular `StreamingRecognitionResult`,
  // the recognizer will not return any further hypotheses for this portion of
  // the transcript and corresponding audio.
  bool is_final = 2;

  // An estimate of the likelihood that the recognizer will not
  // change its guess about this interim result. Values range from 0.0
  // (completely unstable) to 1.0 (completely stable).
  // This field is only provided for interim results (`is_final=false`).
  // The default of 0.0 is a sentinel value indicating `stability` was not set.
  float stability = 3;

  // Time offset of the end of this result relative to the
  // beginning of the audio.
  float result_end_time = 4;

  // For multi-channel audio, this is the channel number corresponding to the
  // recognized result for the audio from that channel.
  // For audio_channel_count = N, its output values can range from '1' to 'N'.
  int32 channel_tag = 5;
}

message SpeechRecognitionAlternative {
  string transcript = 1;
  float confidence = 2;
  repeated WordInfo words = 3;
}

message WordInfo {
  float start_time = 1;
	float end_time = 2;
	string word = 3;
}

message SpeechOperation {
  string name = 1;
  bool done = 2;
  oneof result {
    // If set, returns a [google.rpc.Status][google.rpc.Status] message that
    // specifies the error for the operation.
    google.rpc.Status error = 3;

    LongRunningRecognizeResponse response = 4;
  }
}
